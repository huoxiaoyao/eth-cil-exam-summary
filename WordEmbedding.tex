% !TEX root = Main.tex
\section{Word Embeddings}
\textbf{Distributional Model:}\\
$p_\theta(w|w')$ = Pr[$w$ occurs in context of $w'$]\\
\textbf{Log-likelihood:} $T$ \# of total words\\
$L(\theta; \mathbf{w}) = \sum_{t=1}^T\sum_{\Delta \in I}{\log p_\theta(w^{(t+\Delta)}|w^{(t)})}$\\
\textbf{Latent Vector Model:} $w \rightarrow (\mathbf{x}_w, b_w) \in \mathbb{R}^{D+1} \\p_{\theta}(w|w') = \frac{\exp[\langle \mathbf{x}_w,\mathbf{x}_{w'}\rangle + b_w]}{\sum_{v\in V}{\exp[\langle \mathbf{x}_v,\mathbf{x}_{w'}\rangle + b_v ]}}$ (soft-max). 
$V$ vocabulary, $\theta$s the vectors themself\\
\textbf{Modifications:}\\
$\log p_{\theta}(w|w') = \langle  y_{w} , x_{w'} \rangle + b_w$,  word $y_w$, c'txt $x_{w'}$\\

\subsection*{Negative sampling}
$L(\theta) = \sum_{(i,j) \in \Delta^+}{\log{\sigma(<x_i, y_j>)}} + \sum_{(i,j) \in \Delta^-}{\log {\sigma(-<x_i, y_j>)}}$ where $\Delta^+$ observed couples, $\Delta^-$ neg sampls from distr $P_n(i,j)$\\
opt sol: $<x_i,j_i>^* = h_{i,j}^* = \log{\frac{P(w_i,w_j}{P_n(w_i,w_j)}} + log{\frac{\alpha}{1 - \alpha}}$ where $\alpha = \frac{1}{1+k}$ if $k=1$ \& $P_n(w_i,w_j)=P(w_i)P(w_j)$ then $<x_i,y_j> \approx PMI$ pointwise mutual info


\subsection*{GloVe (Weighted Square Loss)}
\textbf{Co-occurence Matrix:}\\
$\mathbf{N} = (n_{ij}) \in \mathbb{R}^{|V|\times|C|} = \# of word w_i$ in context $w_j$\\
\textbf{Objective:} $H(\theta;\mathbf{N})$\\
$= \sum_{n_{ij} > 0} f(n_{ij})(\log n_{ij} - \tilde{p}_\theta(w_i | w_j))^2$\\
with $f(n) = \min\{1, (\frac{n}{n_{max}})^\alpha\}$, $\alpha \in (0;1]$.\\
$\tilde{p}_\theta(w_i | w_j) = \log \exp[\langle \mathbf{x}_i, \mathbf{y}_j \rangle + b_i + d_j]$ unnormalized distr. $\rightarrow$ 2-sided loss function. If $f=1$ matrix fact prob\\
1. sample $(i,j) u.a.r, s.t. n_{ij}>0$\\
2. $\mathbf{x}_i^{new} \leftarrow \mathbf{x}_i + 2\eta f(n_{ij})(\log n_{ij} - \langle \mathbf{x}_i, \mathbf{y}_j \rangle)\mathbf{y}_j$\\
3. $\mathbf{y}_j^{new} \leftarrow \mathbf{y}_j + 2\eta f(n_{ij})(\log n_{ij} - \langle \mathbf{x}_i, \mathbf{y}_j \rangle)\mathbf{x}_i$
